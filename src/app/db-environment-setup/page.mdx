import BackToTop from "@/components/BackToTop";

# Environment Setup

## Table of Contents

## Overview

For developers, setting up a database environment is crucial for building and testing applications. Your development environment should mirror your production environment as closely as possible to avoid issues when deploying your application.

A well-designed database environment setup provides:

- **Consistency** across development, staging, and production environments
- **Reproducibility** for team members and deployment processes
- **Isolation** between different environments and projects
- **Scalability** to handle growing application demands
- **Security** to protect sensitive data and access controls
- **Efficiency** in development and deployment workflows

## Environment Types and Architecture

### Development Environment Tiers

#### 1. Local Development Environment

```bash
# Typical local development stack
├── Application Code (localhost:3000)
├── Database Server (localhost:5432)
├── Cache Layer (localhost:6379)
└── File Storage (local filesystem)
```

##### Characteristics:

- Runs entirely on developer's machine
- Fast iteration and debugging
- Offline capability
- Full control over configuration
- Limited resources and scalability

##### Setup Example:

```bash
# Local PostgreSQL installation
brew install postgresql
brew services start postgresql

# Create development database
createdb myapp_development

# Connect and verify
psql myapp_development
```

#### 2. Shared Development Environment

```bash
# Shared development infrastructure
├── Shared Database Server (dev-db.company.com)
├── Shared Cache Layer (dev-redis.company.com)
├── Shared File Storage (dev-storage.company.com)
└── Individual Application Instances
```

##### Benefits:

- Shared resources and data
- Team collaboration on database changes
- Realistic network conditions
- Centralized monitoring and backups

##### Configuration:

```yaml
# docker-compose.dev.yml
version: "3.8"
services:
  app:
    build: .
    environment:
      - DATABASE_URL=postgresql://dev-user:password@dev-db.company.com:5432/myapp_dev
      - REDIS_URL=redis://dev-redis.company.com:6379
    volumes:
      - .:/app
    ports:
      - "3000:3000"
```

#### 3. Staging Environment

```bash
# Production-like staging environment
├── Load Balancer (staging-lb.company.com)
├── Application Servers (2+ instances)
├── Database Cluster (master + read replicas)
├── Cache Cluster (Redis/Memcached)
├── Message Queue (RabbitMQ/Apache Kafka)
└── Monitoring Stack (Prometheus + Grafana)
```

##### Purpose:

- Final testing before production deployment
- Performance testing under realistic conditions
- Integration testing with external services
- User acceptance testing (UAT)

#### 4. Production Environment

```bash
# High-availability production setup
├── CDN (CloudFlare/AWS CloudFront)
├── Load Balancers (multiple AZs)
├── Application Clusters (auto-scaling)
├── Database Clusters (multi-AZ with failover)
├── Cache Clusters (Redis Cluster mode)
├── Message Queues (clustered)
├── Monitoring & Alerting
└── Backup & Recovery Systems
```

<BackToTop />
### Environment Isolation Strategies

#### Namespace-Based Isolation

```sql
-- Schema-based isolation in PostgreSQL
CREATE SCHEMA dev_user1;
CREATE SCHEMA dev_user2;
CREATE SCHEMA staging;
CREATE SCHEMA production;

-- Grant appropriate permissions
GRANT USAGE, CREATE ON SCHEMA dev_user1 TO developer1;
GRANT USAGE ON SCHEMA staging TO staging_app;
```

#### Database-Per-Environment

```bash
# Separate databases for isolation
myapp_development    # Local development
myapp_staging        # Staging environment
myapp_production     # Production environment
myapp_test           # Automated testing
```

#### Container-Based Isolation

```yaml
# Docker Compose for isolated environments
version: "3.8"
services:
  db_dev:
    image: postgres:15
    environment:
      - POSTGRES_DB=myapp_dev
      - POSTGRES_USER=dev_user
      - POSTGRES_PASSWORD=dev_password
    volumes:
      - dev_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"

  db_test:
    image: postgres:15
    environment:
      - POSTGRES_DB=myapp_test
      - POSTGRES_USER=test_user
      - POSTGRES_PASSWORD=test_password
    volumes:
      - test_data:/var/lib/postgresql/data
    ports:
      - "5433:5432"

volumes:
  dev_data:
  test_data:
```

<BackToTop />

## Database Platform Selection

### SQL Database Comparison

#### PostgreSQL

```bash
# Installation and setup
# macOS
brew install postgresql
brew services start postgresql

# Ubuntu/Debian
sudo apt update
sudo apt install postgresql postgresql-contrib

# Create user and database
sudo -u postgres createuser --interactive myapp_user
sudo -u postgres createdb myapp_development --owner=myapp_user
```

##### Strengths:

- ACID compliance and reliability
- Advanced features (JSON, arrays, custom types)
- Excellent performance for complex queries
- Strong ecosystem and community support
- Horizontal scaling with extensions

##### Configuration Example:

```ini
# postgresql.conf
listen_addresses = 'localhost'
port = 5432
max_connections = 200
shared_buffers = 256MB
effective_cache_size = 1GB
work_mem = 4MB
maintenance_work_mem = 64MB
```

#### MySQL

```bash
# Installation
# macOS
brew install mysql
brew services start mysql

# Ubuntu/Debian
sudo apt update
sudo apt install mysql-server

# Secure installation
sudo mysql_secure_installation
```

##### Strengths:

- High performance for read-heavy workloads
- Excellent replication capabilities
- Wide hosting support
- Mature ecosystem
- Good for web applications

##### Configuration Example:

```ini
# my.cnf
[mysqld]
bind-address = 127.0.0.1
port = 3306
max_connections = 200
innodb_buffer_pool_size = 256M
innodb_log_file_size = 64M
query_cache_type = 1
query_cache_size = 32M
```

#### SQLite

```bash
# Installation (usually included with Python)
sqlite3 --version

# Create database
sqlite3 myapp_development.db

# Basic operations
.tables
.schema users
.quit
```

##### Use Cases:

- Small to medium applications
- Development and testing
- Embedded applications
- Desktop applications
- Prototyping

<BackToTop />
### NoSQL Database Options

#### MongoDB

```bash
# Installation
# macOS
brew tap mongodb/brew
brew install mongodb-community

# Start service
brew services start mongodb/brew/mongodb-community

# Connect
mongosh
```

##### Setup Example:

```javascript
// MongoDB setup script
use myapp_development;

// Create user
db.createUser({
  user: "myapp_user",
  pwd: "password",
  roles: [
    { role: "readWrite", db: "myapp_development" }
  ]
});

// Create collections with validation
db.createCollection("users", {
  validator: {
    $jsonSchema: {
      bsonType: "object",
      required: ["email", "createdAt"],
      properties: {
        email: {
          bsonType: "string",
          pattern: "^.+@.+$"
        },
        createdAt: {
          bsonType: "date"
        }
      }
    }
  }
});
```

#### Redis

```bash
# Installation
# macOS
brew install redis
brew services start redis

# Ubuntu/Debian
sudo apt install redis-server

# Connect and test
redis-cli
ping
```

##### Configuration:

```ini
# redis.conf
bind 127.0.0.1
port 6379
timeout 300
keepalive 60
maxmemory 256mb
maxmemory-policy allkeys-lru
save 900 1
save 300 10
save 60 10000
```

<BackToTop />

## Local Development Setup

### Single Database Setup

#### PostgreSQL Local Development

```bash
#!/bin/bash
# setup_postgres.sh

# Install PostgreSQL
if command -v brew &> /dev/null; then
    brew install postgresql
    brew services start postgresql
elif command -v apt-get &> /dev/null; then
    sudo apt update
    sudo apt install postgresql postgresql-contrib
    sudo systemctl start postgresql
    sudo systemctl enable postgresql
fi

# Create development user and database
sudo -u postgres psql << EOF
CREATE USER myapp_dev WITH PASSWORD 'dev_password';
CREATE DATABASE myapp_development OWNER myapp_dev;
GRANT ALL PRIVILEGES ON DATABASE myapp_development TO myapp_dev;
ALTER USER myapp_dev CREATEDB;
\q
EOF

echo "PostgreSQL development environment ready!"
echo "Connection string: postgresql://myapp_dev:dev_password@localhost:5432/myapp_development"
```

#### Environment Variables Setup

```bash
# .env.development
DATABASE_URL=postgresql://myapp_dev:dev_password@localhost:5432/myapp_development
DATABASE_HOST=localhost
DATABASE_PORT=5432
DATABASE_NAME=myapp_development
DATABASE_USER=myapp_dev
DATABASE_PASSWORD=dev_password
DATABASE_SSL=false

# Redis configuration
REDIS_URL=redis://localhost:6379
REDIS_HOST=localhost
REDIS_PORT=6379

# Application settings
NODE_ENV=development
PORT=3000
LOG_LEVEL=debug
```

<BackToTop />
### Multi-Database Setup

#### Development Stack with Multiple Databases

```bash
#!/bin/bash
# setup_multi_db.sh

# PostgreSQL for main application data
brew install postgresql
brew services start postgresql

# Redis for caching and sessions
brew install redis
brew services start redis

# MongoDB for document storage
brew tap mongodb/brew
brew install mongodb-community
brew services start mongodb/brew/mongodb-community

# Create PostgreSQL development setup
sudo -u postgres psql << EOF
CREATE USER myapp_dev WITH PASSWORD 'dev_password';
CREATE DATABASE myapp_development OWNER myapp_dev;
CREATE DATABASE myapp_test OWNER myapp_dev;
GRANT ALL PRIVILEGES ON DATABASE myapp_development TO myapp_dev;
GRANT ALL PRIVILEGES ON DATABASE myapp_test TO myapp_dev;
\q
EOF

# Setup MongoDB development database
mongosh << EOF
use myapp_development;
db.createUser({
  user: "myapp_dev",
  pwd: "dev_password",
  roles: ["readWrite"]
});
use myapp_test;
db.createUser({
  user: "myapp_test",
  pwd: "test_password",
  roles: ["readWrite"]
});
EOF

echo "Multi-database development environment ready!"
```

<BackToTop />
### Database GUI Tools Setup

#### pgAdmin for PostgreSQL

```bash
# Install pgAdmin
brew install --cask pgadmin4

# Alternative: Docker-based pgAdmin
docker run --name pgadmin \
  -e PGADMIN_DEFAULT_EMAIL=admin@example.com \
  -e PGADMIN_DEFAULT_PASSWORD=admin \
  -p 8080:80 \
  -d dpage/pgadmin4
```

#### MongoDB Compass

```bash
# Install MongoDB Compass
brew install --cask mongodb-compass

# Connect to local MongoDB
# Connection string: mongodb://myapp_dev:dev_password@localhost:27017/myapp_development
```

<BackToTop />
## Containerization with Docker

### Single Database Container

#### PostgreSQL Docker Setup

```yaml
# docker-compose.yml
version: "3.8"

services:
  postgres:
    image: postgres:15-alpine
    container_name: myapp_postgres
    environment:
      POSTGRES_DB: myapp_development
      POSTGRES_USER: myapp_user
      POSTGRES_PASSWORD: myapp_password
      PGDATA: /var/lib/postgresql/data/pgdata
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./init-scripts:/docker-entrypoint-initdb.d
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U myapp_user -d myapp_development"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

volumes:
  postgres_data:
    driver: local
```

#### Database Initialization Scripts

```sql
-- init-scripts/01-create-extensions.sql
CREATE EXTENSION IF NOT EXISTS "uuid-ossp";
CREATE EXTENSION IF NOT EXISTS "pgcrypto";
CREATE EXTENSION IF NOT EXISTS "pg_stat_statements";

-- init-scripts/02-create-schemas.sql
CREATE SCHEMA IF NOT EXISTS app;
CREATE SCHEMA IF NOT EXISTS audit;
CREATE SCHEMA IF NOT EXISTS temp;

-- Grant permissions
GRANT USAGE, CREATE ON SCHEMA app TO myapp_user;
GRANT USAGE, CREATE ON SCHEMA audit TO myapp_user;
GRANT USAGE, CREATE ON SCHEMA temp TO myapp_user;
```

### Complete Development Stack

#### Full Stack Docker Compose

```yaml
# docker-compose.dev.yml
version: "3.8"

services:
  # Application
  app:
    build:
      context: .
      dockerfile: Dockerfile.dev
    container_name: myapp_app
    environment:
      - NODE_ENV=development
      - DATABASE_URL=postgresql://myapp_user:myapp_password@postgres:5432/myapp_development
      - REDIS_URL=redis://redis:6379
      - MONGODB_URL=mongodb://myapp_user:myapp_password@mongodb:27017/myapp_development
    volumes:
      - .:/app
      - /app/node_modules
    ports:
      - "3000:3000"
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      mongodb:
        condition: service_healthy
    command: npm run dev

  # PostgreSQL Database
  postgres:
    image: postgres:15-alpine
    container_name: myapp_postgres
    environment:
      POSTGRES_DB: myapp_development
      POSTGRES_USER: myapp_user
      POSTGRES_PASSWORD: myapp_password
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./database/init:/docker-entrypoint-initdb.d
      - ./database/backups:/backups
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U myapp_user -d myapp_development"]
      interval: 10s
      timeout: 5s
      retries: 5

  # Redis Cache
  redis:
    image: redis:7-alpine
    container_name: myapp_redis
    command: redis-server --appendonly yes --requirepass redis_password
    volumes:
      - redis_data:/data
      - ./redis/redis.conf:/usr/local/etc/redis/redis.conf
    ports:
      - "6379:6379"
    healthcheck:
      test: ["CMD", "redis-cli", "--raw", "incr", "ping"]
      interval: 10s
      timeout: 3s
      retries: 5

  # MongoDB
  mongodb:
    image: mongo:6
    container_name: myapp_mongodb
    environment:
      MONGO_INITDB_ROOT_USERNAME: root
      MONGO_INITDB_ROOT_PASSWORD: root_password
      MONGO_INITDB_DATABASE: myapp_development
    volumes:
      - mongodb_data:/data/db
      - ./mongodb/init:/docker-entrypoint-initdb.d
    ports:
      - "27017:27017"
    healthcheck:
      test: echo 'db.runCommand("ping").ok' | mongosh localhost:27017/test --quiet
      interval: 10s
      timeout: 10s
      retries: 5

  # pgAdmin
  pgadmin:
    image: dpage/pgadmin4:latest
    container_name: myapp_pgadmin
    environment:
      PGADMIN_DEFAULT_EMAIL: admin@example.com
      PGADMIN_DEFAULT_PASSWORD: admin_password
      PGADMIN_CONFIG_SERVER_MODE: "False"
    volumes:
      - pgadmin_data:/var/lib/pgadmin
      - ./pgadmin/servers.json:/pgadmin4/servers.json
    ports:
      - "8080:80"
    depends_on:
      - postgres

  # MongoDB Express
  mongo-express:
    image: mongo-express:latest
    container_name: myapp_mongo_express
    environment:
      ME_CONFIG_MONGODB_ADMINUSERNAME: root
      ME_CONFIG_MONGODB_ADMINPASSWORD: root_password
      ME_CONFIG_MONGODB_URL: mongodb://root:root_password@mongodb:27017/
      ME_CONFIG_BASICAUTH_USERNAME: admin
      ME_CONFIG_BASICAUTH_PASSWORD: admin_password
    ports:
      - "8081:8081"
    depends_on:
      - mongodb

volumes:
  postgres_data:
  redis_data:
  mongodb_data:
  pgadmin_data:

networks:
  default:
    name: myapp_network
```

#### Development Dockerfile

```dockerfile
# Dockerfile.dev
FROM node:18-alpine

WORKDIR /app

# Install dependencies
COPY package*.json ./
RUN npm ci

# Install development tools
RUN npm install -g nodemon concurrently

# Copy source code
COPY . .

# Expose port
EXPOSE 3000

# Development command with hot reload
CMD ["npm", "run", "dev"]
```

### Docker Management Scripts

#### Development Environment Management

```bash
#!/bin/bash
# scripts/dev-env.sh

case "$1" in
  start)
    echo "Starting development environment..."
    docker-compose -f docker-compose.dev.yml up -d
    ;;
  stop)
    echo "Stopping development environment..."
    docker-compose -f docker-compose.dev.yml down
    ;;
  restart)
    echo "Restarting development environment..."
    docker-compose -f docker-compose.dev.yml restart
    ;;
  logs)
    docker-compose -f docker-compose.dev.yml logs -f "${2:-app}"
    ;;
  shell)
    docker-compose -f docker-compose.dev.yml exec "${2:-app}" sh
    ;;
  db-shell)
    docker-compose -f docker-compose.dev.yml exec postgres psql -U myapp_user -d myapp_development
    ;;
  redis-shell)
    docker-compose -f docker-compose.dev.yml exec redis redis-cli -a redis_password
    ;;
  mongo-shell)
    docker-compose -f docker-compose.dev.yml exec mongodb mongosh -u root -p root_password
    ;;
  reset)
    echo "Resetting development environment..."
    docker-compose -f docker-compose.dev.yml down -v
    docker-compose -f docker-compose.dev.yml up -d
    ;;
  *)
    echo "Usage: $0 {start|stop|restart|logs|shell|db-shell|redis-shell|mongo-shell|reset}"
    exit 1
    ;;
esac
```

#### Database Backup and Restore

```bash
#!/bin/bash
# scripts/db-backup.sh

TIMESTAMP=$(date +%Y%m%d_%H%M%S)
BACKUP_DIR="./database/backups"

# Create backup directory
mkdir -p $BACKUP_DIR

# PostgreSQL backup
echo "Creating PostgreSQL backup..."
docker-compose -f docker-compose.dev.yml exec -T postgres \
  pg_dump -U myapp_user myapp_development > "$BACKUP_DIR/postgres_backup_$TIMESTAMP.sql"

# MongoDB backup
echo "Creating MongoDB backup..."
docker-compose -f docker-compose.dev.yml exec -T mongodb \
  mongodump --username root --password root_password --authenticationDatabase admin \
  --db myapp_development --archive > "$BACKUP_DIR/mongodb_backup_$TIMESTAMP.archive"

# Redis backup
echo "Creating Redis backup..."
docker-compose -f docker-compose.dev.yml exec -T redis \
  redis-cli -a redis_password --rdb - > "$BACKUP_DIR/redis_backup_$TIMESTAMP.rdb"

echo "Backups completed in $BACKUP_DIR"
```

<BackToTop />
## Cloud Database Services

### Amazon Web Services (AWS)

#### RDS (Relational Database Service)

```bash
# AWS CLI setup for RDS
aws rds create-db-instance \
  --db-instance-identifier myapp-dev-postgres \
  --db-instance-class db.t3.micro \
  --engine postgres \
  --engine-version 15.4 \
  --master-username myapp_admin \
  --master-user-password 'SecurePassword123!' \
  --allocated-storage 20 \
  --storage-type gp2 \
  --vpc-security-group-ids sg-12345678 \
  --db-subnet-group-name myapp-dev-subnet-group \
  --backup-retention-period 7 \
  --multi-az false \
  --publicly-accessible true \
  --storage-encrypted false \
  --tags 'Key=Environment,Value=development' 'Key=Project,Value=myapp'
```

#### Terraform Configuration for RDS

```hcl
# terraform/rds.tf
resource "aws_db_instance" "myapp_postgres" {
  identifier = "myapp-${var.environment}-postgres"

  engine         = "postgres"
  engine_version = "15.4"
  instance_class = var.db_instance_class

  allocated_storage     = var.db_allocated_storage
  max_allocated_storage = var.db_max_allocated_storage
  storage_type         = "gp3"
  storage_encrypted    = true

  db_name  = var.db_name
  username = var.db_username
  password = var.db_password

  vpc_security_group_ids = [aws_security_group.rds.id]
  db_subnet_group_name   = aws_db_subnet_group.main.name

  backup_retention_period = var.backup_retention_period
  backup_window          = "03:00-04:00"
  maintenance_window     = "sun:04:00-sun:05:00"

  skip_final_snapshot = var.environment == "development"
  deletion_protection = var.environment == "production"

  performance_insights_enabled = var.environment == "production"
  monitoring_interval         = var.environment == "production" ? 60 : 0

  tags = {
    Name        = "myapp-${var.environment}-postgres"
    Environment = var.environment
    Project     = "myapp"
  }
}

# Security group for RDS
resource "aws_security_group" "rds" {
  name_prefix = "myapp-${var.environment}-rds"
  vpc_id      = aws_vpc.main.id

  ingress {
    from_port   = 5432
    to_port     = 5432
    protocol    = "tcp"
    cidr_blocks = [aws_vpc.main.cidr_block]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }

  tags = {
    Name        = "myapp-${var.environment}-rds-sg"
    Environment = var.environment
  }
}
```

#### ElastiCache for Redis

```hcl
# terraform/elasticache.tf
resource "aws_elasticache_subnet_group" "main" {
  name       = "myapp-${var.environment}-cache-subnet"
  subnet_ids = aws_subnet.private[*].id
}

resource "aws_elasticache_replication_group" "redis" {
  replication_group_id         = "myapp-${var.environment}-redis"
  description                  = "Redis cluster for ${var.environment}"

  node_type                    = var.redis_node_type
  port                         = 6379
  parameter_group_name         = "default.redis7"

  num_cache_clusters           = var.redis_num_cache_nodes

  subnet_group_name            = aws_elasticache_subnet_group.main.name
  security_group_ids           = [aws_security_group.redis.id]

  at_rest_encryption_enabled   = true
  transit_encryption_enabled   = true
  auth_token                   = var.redis_auth_token

  snapshot_retention_limit     = var.environment == "production" ? 7 : 1
  snapshot_window              = "03:00-05:00"

  tags = {
    Name        = "myapp-${var.environment}-redis"
    Environment = var.environment
    Project     = "myapp"
  }
}
```

### Google Cloud Platform (GCP)

#### Cloud SQL Setup

```bash
# Create Cloud SQL instance
gcloud sql instances create myapp-dev-postgres \
  --database-version=POSTGRES_15 \
  --tier=db-f1-micro \
  --region=us-central1 \
  --storage-type=SSD \
  --storage-size=10GB \
  --backup-start-time=03:00 \
  --maintenance-window-day=SUN \
  --maintenance-window-hour=04 \
  --database-flags=shared_preload_libraries=pg_stat_statements

# Create database and user
gcloud sql databases create myapp_development --instance=myapp-dev-postgres
gcloud sql users create myapp_user \
  --instance=myapp-dev-postgres \
  --password=SecurePassword123!
```

#### Terraform Configuration for Cloud SQL

```hcl
# terraform/cloudsql.tf
resource "google_sql_database_instance" "postgres" {
  name             = "myapp-${var.environment}-postgres"
  database_version = "POSTGRES_15"
  region           = var.region

  settings {
    tier              = var.db_tier
    availability_type = var.environment == "production" ? "REGIONAL" : "ZONAL"
    disk_type         = "PD_SSD"
    disk_size         = var.db_disk_size
    disk_autoresize   = true

    backup_configuration {
      enabled                        = true
      start_time                     = "03:00"
      point_in_time_recovery_enabled = var.environment == "production"
      backup_retention_settings {
        retained_backups = var.environment == "production" ? 30 : 7
      }
    }

    maintenance_window {
      day  = 7
      hour = 4
    }

    database_flags {
      name  = "shared_preload_libraries"
      value = "pg_stat_statements"
    }

    ip_configuration {
      ipv4_enabled    = true
      authorized_networks {
        name  = "allow-all"
        value = "0.0.0.0/0"
      }
    }
  }

  deletion_protection = var.environment == "production"
}

resource "google_sql_database" "database" {
  name     = var.db_name
  instance = google_sql_database_instance.postgres.name
}

resource "google_sql_user" "user" {
  name     = var.db_username
  instance = google_sql_database_instance.postgres.name
  password = var.db_password
}
```

### Microsoft Azure

#### Azure Database for PostgreSQL

```bash
# Create Azure Database for PostgreSQL
az postgres server create \
  --resource-group myapp-dev-rg \
  --name myapp-dev-postgres \
  --location eastus \
  --admin-user myapp_admin \
  --admin-password 'SecurePassword123!' \
  --sku-name GP_Gen5_2 \
  --version 15 \
  --storage-size 5120 \
  --backup-retention 7 \
  --geo-redundant-backup Disabled

# Create database
az postgres db create \
  --resource-group myapp-dev-rg \
  --server-name myapp-dev-postgres \
  --name myapp_development

# Configure firewall
az postgres server firewall-rule create \
  --resource-group myapp-dev-rg \
  --server myapp-dev-postgres \
  --name AllowAllIps \
  --start-ip-address 0.0.0.0 \
  --end-ip-address 255.255.255.255
```

### Connection String Management

#### Environment-Specific Connection Strings

```bash
# .env.development
DATABASE_URL=postgresql://username:password@localhost:5432/myapp_development

# .env.staging
DATABASE_URL=postgresql://username:password@staging-db.company.com:5432/myapp_staging

# .env.production
DATABASE_URL=postgresql://username:password@prod-cluster.company.com:5432/myapp_production

# AWS RDS connection
DATABASE_URL=postgresql://myapp_admin:SecurePassword123!@myapp-prod-postgres.cluster-abc123.us-east-1.rds.amazonaws.com:5432/myapp_production

# Google Cloud SQL connection
DATABASE_URL=postgresql://myapp_user:SecurePassword123!@/myapp_production?host=/cloudsql/project-id:region:instance-name

# Azure Database connection
DATABASE_URL=postgresql://myapp_admin@myapp-dev-postgres:SecurePassword123!@myapp-dev-postgres.postgres.database.azure.com:5432/myapp_development
```

<BackToTop />

## Configuration Management

## Configuration Management

### Environment Variables Strategy

#### Hierarchical Configuration

```javascript
// config/database.js
const config = {
  development: {
    host: process.env.DB_HOST || "localhost",
    port: parseInt(process.env.DB_PORT || "5432"),
    database: process.env.DB_NAME || "myapp_development",
    username: process.env.DB_USER || "myapp_dev",
    password: process.env.DB_PASSWORD || "dev_password",
    dialect: "postgres",
    logging: console.log,
    pool: {
      max: 5,
      min: 0,
      acquire: 30000,
      idle: 10000,
    },
  },

  test: {
    host: process.env.DB_HOST || "localhost",
    port: parseInt(process.env.DB_PORT || "5433"),
    database: process.env.DB_NAME || "myapp_test",
    username: process.env.DB_USER || "myapp_test",
    password: process.env.DB_PASSWORD || "test_password",
    dialect: "postgres",
    logging: false, // Disable logging in tests
    pool: {
      max: 5,
      min: 0,
      acquire: 30000,
      idle: 10000,
    },
  },

  staging: {
    host: process.env.DB_HOST,
    port: parseInt(process.env.DB_PORT || "5432"),
    database: process.env.DB_NAME,
    username: process.env.DB_USER,
    password: process.env.DB_PASSWORD,
    dialect: "postgres",
    logging: false,
    ssl: {
      require: true,
      rejectUnauthorized: false,
    },
    pool: {
      max: 20,
      min: 5,
      acquire: 60000,
      idle: 10000,
    },
  },

  production: {
    host: process.env.DB_HOST,
    port: parseInt(process.env.DB_PORT || "5432"),
    database: process.env.DB_NAME,
    username: process.env.DB_USER,
    password: process.env.DB_PASSWORD,
    dialect: "postgres",
    logging: false,
    ssl: {
      require: true,
      rejectUnauthorized: true,
      ca: process.env.DB_SSL_CA,
      cert: process.env.DB_SSL_CERT,
      key: process.env.DB_SSL_KEY,
    },
    pool: {
      max: 50,
      min: 10,
      acquire: 60000,
      idle: 10000,
    },
  },
};

module.exports = config[process.env.NODE_ENV || "development"];
```

<BackToTop />
#### Configuration Validation ```javascript // config/validation.js const Joi =
require('joi');

const envSchema = Joi.object({
NODE_ENV: Joi.string()
.valid('development', 'test', 'staging', 'production')
.default('development'),

// Database configuration
DB_HOST: Joi.string().required(),
DB_PORT: Joi.number().port().default(5432),
DB_NAME: Joi.string().required(),
DB_USER: Joi.string().required(),
DB_PASSWORD: Joi.string().required(),

// SSL configuration (required for production)
DB_SSL_CA: Joi.when('NODE_ENV', {
is: 'production',
then: Joi.string().required(),
otherwise: Joi.string().optional()
}),

// Redis configuration
REDIS_URL: Joi.string().uri().required(),
REDIS_PASSWORD: Joi.string().when('NODE_ENV', {
is: Joi.string().valid('staging', 'production'),
then: Joi.required(),
otherwise: Joi.optional()
}),

// Application configuration
PORT: Joi.number().port().default(3000),
LOG_LEVEL: Joi.string()
.valid('error', 'warn', 'info', 'debug')
.default('info'),

// Security
JWT_SECRET: Joi.string().min(32).required(),
ENCRYPTION_KEY: Joi.string().length(32).required()
}).unknown();

const { error, value: envVars } = envSchema.validate(process.env);

if (error) {
throw new Error(`Config validation error: ${error.message}`);
}

module.exports = envVars;

````

<BackToTop />

### Secrets Management

#### AWS Secrets Manager Integration
```javascript
// utils/secrets.js
const AWS = require('aws-sdk');

class SecretsManager {
  constructor() {
    this.client = new AWS.SecretsManager({
      region: process.env.AWS_REGION || 'us-east-1'
    });
  }

  async getSecret(secretName) {
    try {
      const result = await this.client.getSecretValue({
        SecretId: secretName
      }).promise();

      return JSON.parse(result.SecretString);
    } catch (error) {
      console.error(`Error retrieving secret ${secretName}:`, error);
      throw error;
    }
  }

  async getDatabaseCredentials(environment) {
    const secretName = `myapp/${environment}/database`;
    return await this.getSecret(secretName);
  }
}

// config/database-with-secrets.js
const SecretsManager = require('../utils/secrets');

async function getDatabaseConfig() {
  if (process.env.NODE_ENV === 'production') {
    const secretsManager = new SecretsManager();
    const credentials = await secretsManager.getDatabaseCredentials('production');

    return {
      host: credentials.host,
      port: credentials.port,
      database: credentials.database,
      username: credentials.username,
      password: credentials.password,
      ssl: {
        require: true,
        rejectUnauthorized: true,
        ca: credentials.ssl_ca
      }
    };
  }

  // Use environment variables for development
  return require('./database');
}

module.exports = getDatabaseConfig;
````

<BackToTop />
#### HashiCorp Vault Integration ```javascript // utils/vault.js const vault =
require('node-vault');

class VaultClient {
constructor() {
this.client = vault({
apiVersion: 'v1',
endpoint: process.env.VAULT_ENDPOINT,
token: process.env.VAULT_TOKEN
});
}

async getDatabaseCredentials(environment) {
try {
const result = await this.client.read(`secret/data/myapp/${environment}/database`);
return result.data.data;
} catch (error) {
console.error('Error reading from Vault:', error);
throw error;
}
}

async getTemporaryDatabaseCredentials(environment, ttl = '1h') {
try {
const result = await this.client.read(`database/creds/myapp-${environment}-role`);
return {
username: result.data.username,
password: result.data.password,
lease_duration: result.lease_duration
};
} catch (error) {
console.error('Error getting temporary credentials:', error);
throw error;
}
}
}

module.exports = VaultClient;

````

<BackToTop />

### Configuration Files Management

#### Database Configuration Files
```yaml
# config/database.yml
default: &default
  adapter: postgresql
  encoding: unicode
  pool: <%= ENV.fetch('DB_POOL', 5) %>
  timeout: 5000
  prepared_statements: false

development:
  <<: *default
  host: <%= ENV.fetch('DB_HOST', 'localhost') %>
  port: <%= ENV.fetch('DB_PORT', 5432) %>
  database: <%= ENV.fetch('DB_NAME', 'myapp_development') %>
  username: <%= ENV.fetch('DB_USER', 'myapp_dev') %>
  password: <%= ENV.fetch('DB_PASSWORD', 'dev_password') %>

test:
  <<: *default
  host: <%= ENV.fetch('DB_HOST', 'localhost') %>
  port: <%= ENV.fetch('DB_PORT', 5433) %>
  database: <%= ENV.fetch('DB_NAME', 'myapp_test') %>
  username: <%= ENV.fetch('DB_USER', 'myapp_test') %>
  password: <%= ENV.fetch('DB_PASSWORD', 'test_password') %>

staging:
  <<: *default
  host: <%= ENV.fetch('DB_HOST') %>
  port: <%= ENV.fetch('DB_PORT', 5432) %>
  database: <%= ENV.fetch('DB_NAME') %>
  username: <%= ENV.fetch('DB_USER') %>
  password: <%= ENV.fetch('DB_PASSWORD') %>
  sslmode: require
  pool: 20

production:
  <<: *default
  host: <%= ENV.fetch('DB_HOST') %>
  port: <%= ENV.fetch('DB_PORT', 5432) %>
  database: <%= ENV.fetch('DB_NAME') %>
  username: <%= ENV.fetch('DB_USER') %>
  password: <%= ENV.fetch('DB_PASSWORD') %>
  sslmode: require
  sslcert: <%= ENV.fetch('DB_SSL_CERT') %>
  sslkey: <%= ENV.fetch('DB_SSL_KEY') %>
  sslrootcert: <%= ENV.fetch('DB_SSL_CA') %>
  pool: 50
````

<BackToTop />
#### Environment-Specific Configurations ```bash # .env.development
NODE_ENV=development PORT=3000 LOG_LEVEL=debug

# Database

DB_HOST=localhost
DB_PORT=5432
DB_NAME=myapp_development
DB_USER=myapp_dev
DB_PASSWORD=dev_password

# Redis

REDIS_URL=redis://localhost:6379

# .env.staging

NODE_ENV=staging
PORT=3000
LOG_LEVEL=info

# Database

DB_HOST=staging-db.company.com
DB_PORT=5432
DB_NAME=myapp_staging
DB_USER=myapp_staging
DB_PASSWORD=${STAGING_DB_PASSWORD}

# Redis

REDIS_URL=redis://:${STAGING_REDIS_PASSWORD}@staging-redis.company.com:6379

# .env.production

NODE_ENV=production
PORT=3000
LOG_LEVEL=warn

# Database (loaded from secrets manager)

DB_HOST=${PROD_DB_HOST}
DB_PORT=5432
DB_NAME=myapp_production
DB_USER=${PROD_DB_USER}
DB_PASSWORD=${PROD_DB_PASSWORD}
DB_SSL_CA=${PROD_DB_SSL_CA}
DB_SSL_CERT=${PROD_DB_SSL_CERT}
DB_SSL_KEY=${PROD_DB_SSL_KEY}

# Redis

REDIS_URL=${PROD_REDIS_URL}

````

<BackToTop />

## Data Seeding and Fixtures

### Database Seeding Strategies

#### SQL-Based Seeding
```sql
-- seeds/001_users.sql
INSERT INTO users (id, email, first_name, last_name, role, created_at, updated_at) VALUES
  (1, 'admin@example.com', 'Admin', 'User', 'admin', NOW(), NOW()),
  (2, 'user@example.com', 'Regular', 'User', 'user', NOW(), NOW()),
  (3, 'manager@example.com', 'Manager', 'User', 'manager', NOW(), NOW())
ON CONFLICT (email) DO NOTHING;

-- seeds/002_categories.sql
INSERT INTO categories (id, name, description, parent_id, created_at, updated_at) VALUES
  (1, 'Electronics', 'Electronic devices and accessories', NULL, NOW(), NOW()),
  (2, 'Computers', 'Desktop and laptop computers', 1, NOW(), NOW()),
  (3, 'Mobile Phones', 'Smartphones and accessories', 1, NOW(), NOW()),
  (4, 'Books', 'Physical and digital books', NULL, NOW(), NOW()),
  (5, 'Fiction', 'Fiction books and novels', 4, NOW(), NOW())
ON CONFLICT (name) DO NOTHING;

-- seeds/003_products.sql
INSERT INTO products (id, name, description, price, category_id, sku, stock_quantity, created_at, updated_at) VALUES
  (1, 'MacBook Pro 16"', 'Apple MacBook Pro with M2 chip', 2499.00, 2, 'MBP-16-M2', 10, NOW(), NOW()),
  (2, 'iPhone 15 Pro', 'Latest iPhone with A17 Pro chip', 999.00, 3, 'IP15-PRO', 25, NOW(), NOW()),
  (3, 'The Great Gatsby', 'Classic American novel', 12.99, 5, 'TGG-CLASSIC', 100, NOW(), NOW()),
  (4, 'Dell XPS 13', 'Ultra-thin laptop with Intel i7', 1299.00, 2, 'DELL-XPS13', 15, NOW(), NOW())
ON CONFLICT (sku) DO NOTHING;
````

<BackToTop />
#### JavaScript/Node.js Seeding ```javascript // seeds/index.js const
{(Sequelize, DataTypes)} = require('sequelize'); const config =
require('../config/database');

const sequelize = new Sequelize(config);

// Define models
const User = require('../models/User')(sequelize, DataTypes);
const Category = require('../models/Category')(sequelize, DataTypes);
const Product = require('../models/Product')(sequelize, DataTypes);

// Define associations
Category.hasMany(Category, { as: 'children', foreignKey: 'parent_id' });
Category.belongsTo(Category, { as: 'parent', foreignKey: 'parent_id' });
Category.hasMany(Product, { foreignKey: 'category_id' });
Product.belongsTo(Category, { foreignKey: 'category_id' });

const seedData = {
users: [
{
email: 'admin@example.com',
firstName: 'Admin',
lastName: 'User',
role: 'admin',
passwordHash: '$2b$10$...' // Pre-hashed password
},
{
email: 'user@example.com',
firstName: 'Regular',
lastName: 'User',
role: 'user',
passwordHash: '$2b$10$...'
}
],

categories: [
{
id: 1,
name: 'Electronics',
description: 'Electronic devices and accessories',
parentId: null
},
{
id: 2,
name: 'Computers',
description: 'Desktop and laptop computers',
parentId: 1
},
{
id: 3,
name: 'Mobile Phones',
description: 'Smartphones and accessories',
parentId: 1
}
],

products: [
{
name: 'MacBook Pro 16"',
description: 'Apple MacBook Pro with M2 chip',
price: 2499.00,
categoryId: 2,
sku: 'MBP-16-M2',
stockQuantity: 10
},
{
name: 'iPhone 15 Pro',
description: 'Latest iPhone with A17 Pro chip',
price: 999.00,
categoryId: 3,
sku: 'IP15-PRO',
stockQuantity: 25
}
]
};

async function seedDatabase() {
try {
// Sync database (create tables)
await sequelize.sync({ force: process.env.NODE_ENV === 'development' });

    console.log('Seeding users...');
    for (const userData of seedData.users) {
      await User.findOrCreate({
        where: { email: userData.email },
        defaults: userData
      });
    }

    console.log('Seeding categories...');
    for (const categoryData of seedData.categories) {
      await Category.findOrCreate({
        where: { name: categoryData.name },
        defaults: categoryData
      });
    }

    console.log('Seeding products...');
    for (const productData of seedData.products) {
      await Product.findOrCreate({
        where: { sku: productData.sku },
        defaults: productData
      });
    }

    console.log('Database seeded successfully!');

} catch (error) {
console.error('Error seeding database:', error);
throw error;
} finally {
await sequelize.close();
}
}

// Run seeding if called directly
if (require.main === module) {
seedDatabase()
.then(() => process.exit(0))
.catch(() => process.exit(1));
}

module.exports = seedDatabase;

````


<BackToTop />
#### Factory-Based Data Generation
```javascript
// factories/userFactory.js
const { faker } = require('@faker-js/faker');
const bcrypt = require('bcrypt');

class UserFactory {
  static async create(overrides = {}) {
    const defaultData = {
      email: faker.internet.email(),
      firstName: faker.person.firstName(),
      lastName: faker.person.lastName(),
      passwordHash: await bcrypt.hash('password123', 10),
      role: faker.helpers.arrayElement(['user', 'admin', 'manager']),
      isActive: faker.datatype.boolean({ probability: 0.9 }),
      lastLoginAt: faker.date.recent({ days: 30 }),
      createdAt: faker.date.past({ years: 2 }),
      profile: {
        bio: faker.lorem.paragraph(),
        avatar: faker.image.avatar(),
        location: faker.location.city(),
        website: faker.internet.url()
      }
    };

    return { ...defaultData, ...overrides };
  }

  static async createMany(count, overrides = {}) {
    const users = [];
    for (let i = 0; i < count; i++) {
      users.push(await this.create(overrides));
    }
    return users;
  }

  static admin(overrides = {}) {
    return this.create({ role: 'admin', ...overrides });
  }

  static regularUser(overrides = {}) {
    return this.create({ role: 'user', ...overrides });
  }
}

// factories/productFactory.js
class ProductFactory {
  static create(overrides = {}) {
    const categories = ['Electronics', 'Books', 'Clothing', 'Home & Garden', 'Sports'];

    const defaultData = {
      name: faker.commerce.productName(),
      description: faker.commerce.productDescription(),
      price: parseFloat(faker.commerce.price({ min: 10, max: 1000 })),
      sku: faker.string.alphanumeric({ length: 8, casing: 'upper' }),
      stockQuantity: faker.number.int({ min: 0, max: 100 }),
      weight: faker.number.float({ min: 0.1, max: 50, fractionDigits: 2 }),
      dimensions: {
        length: faker.number.float({ min: 1, max: 100, fractionDigits: 1 }),
        width: faker.number.float({ min: 1, max: 100, fractionDigits: 1 }),
        height: faker.number.float({ min: 1, max: 100, fractionDigits: 1 })
      },
      category: faker.helpers.arrayElement(categories),
      isActive: faker.datatype.boolean({ probability: 0.95 }),
      tags: faker.helpers.arrayElements(
        ['new', 'popular', 'sale', 'featured', 'limited'],
        { min: 0, max: 3 }
      )
    };

    return { ...defaultData, ...overrides };
  }

  static createMany(count, overrides = {}) {
    return Array.from({ length: count }, () => this.create(overrides));
  }
}

// seeds/factories.js
const { User, Product, Order, OrderItem } = require('../models');
const UserFactory = require('../factories/userFactory');
const ProductFactory = require('../factories/productFactory');

async function seedWithFactories() {
  console.log('Creating users...');
  const users = await UserFactory.createMany(50);
  const createdUsers = await User.bulkCreate(users);

  console.log('Creating products...');
  const products = ProductFactory.createMany(100);
  const createdProducts = await Product.bulkCreate(products);

  console.log('Creating orders...');
  const orders = [];
  for (let i = 0; i < 200; i++) {
    const user = faker.helpers.arrayElement(createdUsers);
    const orderProducts = faker.helpers.arrayElements(createdProducts, { min: 1, max: 5 });

    const order = await Order.create({
      userId: user.id,
      status: faker.helpers.arrayElement(['pending', 'confirmed', 'shipped', 'delivered']),
      totalAmount: 0 // Will be calculated
    });

    let totalAmount = 0;
    for (const product of orderProducts) {
      const quantity = faker.number.int({ min: 1, max: 3 });
      const unitPrice = product.price;

      await OrderItem.create({
        orderId: order.id,
        productId: product.id,
        quantity,
        unitPrice,
        lineTotal: quantity * unitPrice
      });

      totalAmount += quantity * unitPrice;
    }

    await order.update({ totalAmount });
  }

  console.log('Factory seeding completed!');
}

module.exports = seedWithFactories;
````

<BackToTop />

### Test Data Management

#### Test Fixtures

```javascript
// fixtures/testData.js
const testData = {
  users: {
    admin: {
      email: "admin@test.com",
      firstName: "Admin",
      lastName: "User",
      role: "admin",
      passwordHash: "$2b$10$testAdminHash",
    },

    regularUser: {
      email: "user@test.com",
      firstName: "Regular",
      lastName: "User",
      role: "user",
      passwordHash: "$2b$10$testUserHash",
    },

    manager: {
      email: "manager@test.com",
      firstName: "Manager",
      lastName: "User",
      role: "manager",
      passwordHash: "$2b$10$testManagerHash",
    },
  },

  products: {
    laptop: {
      name: "Test Laptop",
      description: "A laptop for testing",
      price: 999.99,
      sku: "TEST-LAPTOP-001",
      stockQuantity: 10,
      categoryId: 1,
    },

    phone: {
      name: "Test Phone",
      description: "A phone for testing",
      price: 599.99,
      sku: "TEST-PHONE-001",
      stockQuantity: 5,
      categoryId: 2,
    },
  },

  orders: {
    pending: {
      status: "pending",
      totalAmount: 999.99,
      items: [
        {
          productSku: "TEST-LAPTOP-001",
          quantity: 1,
          unitPrice: 999.99,
        },
      ],
    },

    completed: {
      status: "delivered",
      totalAmount: 1599.98,
      items: [
        {
          productSku: "TEST-LAPTOP-001",
          quantity: 1,
          unitPrice: 999.99,
        },
        {
          productSku: "TEST-PHONE-001",
          quantity: 1,
          unitPrice: 599.99,
        },
      ],
    },
  },
};

module.exports = testData;
```

<BackToTop />
#### Test Database Setup and Teardown ```javascript // tests/helpers/database.js
const {Sequelize} = require('sequelize'); const config =
require('../../config/database'); const models = require('../../models'); const
testData = require('../../fixtures/testData');

class TestDatabase {
constructor() {
this.sequelize = new Sequelize(config.test);
this.models = models(this.sequelize);
}

async setup() {
try {
// Create all tables
await this.sequelize.sync({ force: true });

      // Seed basic test data
      await this.seedTestData();

      console.log('Test database setup completed');
    } catch (error) {
      console.error('Test database setup failed:', error);
      throw error;
    }

}

async teardown() {
try {
// Clean up all data
await this.sequelize.drop();
await this.sequelize.close();

      console.log('Test database teardown completed');
    } catch (error) {
      console.error('Test database teardown failed:', error);
      throw error;
    }

}

async seedTestData() {
// Create test users
for (const [key, userData] of Object.entries(testData.users)) {
await this.models.User.create(userData);
}

    // Create test categories
    await this.models.Category.bulkCreate([
      { id: 1, name: 'Electronics' },
      { id: 2, name: 'Mobile Phones' }
    ]);

    // Create test products
    for (const [key, productData] of Object.entries(testData.products)) {
      await this.models.Product.create(productData);
    }

}

async cleanTable(tableName) {
await this.models[tableName].destroy({ where: {}, truncate: true });
}

async resetSequences() {
// Reset auto-increment sequences (PostgreSQL)
const tables = Object.keys(this.models);
for (const table of tables) {
await this.sequelize.query(
`ALTER SEQUENCE ${table.toLowerCase()}s_id_seq RESTART WITH 1`
);
}
}
}

// Global test setup/teardown
let testDb;

async function setupTestDatabase() {
testDb = new TestDatabase();
await testDb.setup();
return testDb;
}

async function teardownTestDatabase() {
if (testDb) {
await testDb.teardown();
}
}

module.exports = {
TestDatabase,
setupTestDatabase,
teardownTestDatabase,
testData
};

````

<BackToTop />

## Database Schema Management

### Migration Systems

#### Sequelize Migrations
```javascript
// migrations/20240101000001-create-users.js
'use strict';

module.exports = {
  async up(queryInterface, Sequelize) {
    await queryInterface.createTable('users', {
      id: {
        allowNull: false,
        autoIncrement: true,
        primaryKey: true,
        type: Sequelize.INTEGER
      },
      email: {
        type: Sequelize.STRING(100),
        allowNull: false,
        unique: true
      },
      first_name: {
        type: Sequelize.STRING(50),
        allowNull: false
      },
      last_name: {
        type: Sequelize.STRING(50),
        allowNull: false
      },
      password_hash: {
        type: Sequelize.STRING(255),
        allowNull: false
      },
      role: {
        type: Sequelize.ENUM('user', 'admin', 'manager'),
        allowNull: false,
        defaultValue: 'user'
      },
      is_active: {
        type: Sequelize.BOOLEAN,
        allowNull: false,
        defaultValue: true
      },
      last_login_at: {
        type: Sequelize.DATE,
        allowNull: true
      },
      created_at: {
        allowNull: false,
        type: Sequelize.DATE,
        defaultValue: Sequelize.literal('CURRENT_TIMESTAMP')
      },
      updated_at: {
        allowNull: false,
        type: Sequelize.DATE,
        defaultValue: Sequelize.literal('CURRENT_TIMESTAMP')
      }
    });

    // Create indexes
    await queryInterface.addIndex('users', ['email'], {
      unique: true,
      name: 'users_email_unique_idx'
    });

    await queryInterface.addIndex('users', ['role'], {
      name: 'users_role_idx'
    });

    await queryInterface.addIndex('users', ['is_active'], {
      name: 'users_active_idx'
    });
  },

  async down(queryInterface, Sequelize) {
    await queryInterface.dropTable('users');
  }
};
````

<BackToTop />
#### TypeORM Migrations ```typescript // migrations/1640000000000-CreateUsers.ts
import {(MigrationInterface, QueryRunner, Table, Index)} from 'typeorm';

export class CreateUsers1640000000000 implements MigrationInterface {
  public async up(queryRunner: QueryRunner): Promise<void> {
    await queryRunner.createTable(
      new Table({
        name: 'users',
        columns: [
          {
            name: 'id',
            type: 'int',
            isPrimary: true,
            isGenerated: true,
            generationStrategy: 'increment'
          },
          {
            name: 'email',
            type: 'varchar',
            length: '100',
            isUnique: true,
            isNullable: false
          },
          {
            name: 'firstName',
            type: 'varchar',
            length: '50',
            isNullable: false
          },
          {
            name: 'lastName',
            type: 'varchar',
            length: '50',
            isNullable: false
          },
          {
            name: 'passwordHash',
            type: 'varchar',
            length: '255',
            isNullable: false
          },
          {
            name: 'role',
            type: 'enum',
            enum: ['user', 'admin', 'manager'],
            default: "'user'",
            isNullable: false
          },
          {
            name: 'isActive',
            type: 'boolean',
            default: true,
            isNullable: false
          },
          {
            name: 'lastLoginAt',
            type: 'timestamp',
            isNullable: true
          },
          {
            name: 'createdAt',
            type: 'timestamp',
            default: 'CURRENT_TIMESTAMP',
            isNullable: false
          },
          {
            name: 'updatedAt',
            type: 'timestamp',
            default: 'CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP',
            isNullable: false
          }
        ]
      }),
      true
    );
    
    // Create indexes
    await queryRunner.createIndex('users', new Index('IDX_USER_EMAIL', ['email']));
    await queryRunner.createIndex('users', new Index('IDX_USER_ROLE', ['role']));
    await queryRunner.createIndex('users', new Index('IDX_USER_ACTIVE', ['isActive']));
  }

public async down(queryRunner: QueryRunner): Promise<void> {
await queryRunner.dropTable('users');
}
}

````


<BackToTop />
#### Prisma Schema Management
```prisma
// prisma/schema.prisma
generator client {
  provider = "prisma-client-js"
}

datasource db {
  provider = "postgresql"
  url      = env("DATABASE_URL")
}

model User {
  id          Int      @id @default(autoincrement())
  email       String   @unique @db.VarChar(100)
  firstName   String   @map("first_name") @db.VarChar(50)
  lastName    String   @map("last_name") @db.VarChar(50)
  passwordHash String  @map("password_hash") @db.VarChar(255)
  role        Role     @default(USER)
  isActive    Boolean  @default(true) @map("is_active")
  lastLoginAt DateTime? @map("last_login_at")
  createdAt   DateTime @default(now()) @map("created_at")
  updatedAt   DateTime @updatedAt @map("updated_at")

  // Relations
  orders      Order[]
  profile     Profile?

  @@index([email])
  @@index([role])
  @@index([isActive])
  @@map("users")
}

model Profile {
  id       Int     @id @default(autoincrement())
  userId   Int     @unique @map("user_id")
  bio      String?
  avatar   String?
  location String?
  website  String?

  user User @relation(fields: [userId], references: [id], onDelete: Cascade)

  @@map("profiles")
}

model Order {
  id          Int         @id @default(autoincrement())
  userId      Int         @map("user_id")
  orderNumber String      @unique @map("order_number") @db.VarChar(50)
  status      OrderStatus @default(PENDING)
  totalAmount Decimal     @map("total_amount") @db.Decimal(10, 2)
  createdAt   DateTime    @default(now()) @map("created_at")
  updatedAt   DateTime    @updatedAt @map("updated_at")

  user  User        @relation(fields: [userId], references: [id])
  items OrderItem[]

  @@index([userId])
  @@index([status])
  @@index([createdAt])
  @@map("orders")
}

model OrderItem {
  id        Int     @id @default(autoincrement())
  orderId   Int     @map("order_id")
  productId Int     @map("product_id")
  quantity  Int
  unitPrice Decimal @map("unit_price") @db.Decimal(10, 2)
  lineTotal Decimal @map("line_total") @db.Decimal(10, 2)

  order   Order   @relation(fields: [orderId], references: [id], onDelete: Cascade)
  product Product @relation(fields: [productId], references: [id])

  @@unique([orderId, productId])
  @@map("order_items")
}

model Product {
  id            Int         @id @default(autoincrement())
  name          String      @db.VarChar(200)
  description   String?
  price         Decimal     @db.Decimal(10, 2)
  sku           String      @unique @db.VarChar(50)
  stockQuantity Int         @default(0) @map("stock_quantity")
  isActive      Boolean     @default(true) @map("is_active")
  createdAt     DateTime    @default(now()) @map("created_at")
  updatedAt     DateTime    @updatedAt @map("updated_at")

  orderItems OrderItem[]

  @@index([sku])
  @@index([isActive])
  @@map("products")
}

enum Role {
  USER
  ADMIN
  MANAGER
}

enum OrderStatus {
  PENDING
  CONFIRMED
  PROCESSING
  SHIPPED
  DELIVERED
  CANCELLED
}
````

<BackToTop />

### Schema Version Control

#### Migration Management Scripts

```bash
#!/bin/bash
# scripts/migrate.sh

set -e

ENVIRONMENT=${1:-development}
ACTION=${2:-up}

echo "Running migrations for environment: $ENVIRONMENT"

case $ACTION in
  up)
    echo "Running pending migrations..."
    npx sequelize-cli db:migrate --env $ENVIRONMENT
    ;;
  down)
    echo "Rolling back last migration..."
    npx sequelize-cli db:migrate:undo --env $ENVIRONMENT
    ;;
  reset)
    echo "Rolling back all migrations..."
    npx sequelize-cli db:migrate:undo:all --env $ENVIRONMENT
    echo "Running all migrations..."
    npx sequelize-cli db:migrate --env $ENVIRONMENT
    ;;
  status)
    echo "Migration status:"
    npx sequelize-cli db:migrate:status --env $ENVIRONMENT
    ;;
  seed)
    echo "Running seeders..."
    npx sequelize-cli db:seed:all --env $ENVIRONMENT
    ;;
  *)
    echo "Usage: $0 <environment> {up|down|reset|status|seed}"
    echo "  environment: development, test, staging, production"
    echo "  action: up (default), down, reset, status, seed"
    exit 1
    ;;
esac

echo "Migration completed successfully!"
```

<BackToTop />
#### CI/CD Pipeline Integration ```yaml #
.github/workflows/database-migrations.yml name: Database Migrations

on:
push:
branches: [main, develop]
paths: - 'migrations/**' - 'models/**' - 'seeders/**'
pull_request:
branches: [main]
paths: - 'migrations/**' - 'models/**' - 'seeders/**'

jobs:
test-migrations:
runs-on: ubuntu-latest

    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: myapp_test
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

    steps:
      - uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '18'
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Run migrations on fresh database
        env:
          DATABASE_URL: postgresql://postgres:postgres@localhost:5432/myapp_test
        run: |
          npm run migrate:up
          npm run seed:test

      - name: Test migration rollback
        env:
          DATABASE_URL: postgresql://postgres:postgres@localhost:5432/myapp_test
        run: |
          npm run migrate:down
          npm run migrate:up

      - name: Run tests with migrated database
        env:
          DATABASE_URL: postgresql://postgres:postgres@localhost:5432/myapp_test
        run: npm test

deploy-staging:
needs: test-migrations
if: github.ref == 'refs/heads/develop'
runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4

      - name: Deploy to staging
        env:
          STAGING_DATABASE_URL: ${{ secrets.STAGING_DATABASE_URL }}
        run: |
          # Run migrations on staging
          npm run migrate:up -- --env staging

          # Optionally seed staging data
          npm run seed:staging

deploy-production:
needs: test-migrations
if: github.ref == 'refs/heads/main'
runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4

      - name: Create database backup
        env:
          PRODUCTION_DATABASE_URL: ${{ secrets.PRODUCTION_DATABASE_URL }}
        run: |
          # Create backup before migration
          npm run backup:create -- --env production

      - name: Deploy to production
        env:
          PRODUCTION_DATABASE_URL: ${{ secrets.PRODUCTION_DATABASE_URL }}
        run: |
          # Run migrations on production
          npm run migrate:up -- --env production

          # Verify deployment
          npm run migrate:status -- --env production

```

<BackToTop />

## Things to Consider

- **Database Type**: Choose between SQL (e.g., PostgreSQL, MySQL) and NoSQL (e.g., MongoDB, Redis) based on your application needs.
- **Local vs. Cloud**: Decide whether to run your database locally or use a cloud service. Local databases are easier to set up for development, while cloud databases offer scalability and remote access.
- **Development Tools**: Use database management tools (e.g., pgAdmin for PostgreSQL, MongoDB Compass for MongoDB) to simplify database management tasks.
- **Environment Configuration**:
  - Ensure your database configuration files (e.g., connection strings, credentials) are secure and not hard-coded in your application.
  - Use environment variables to manage sensitive information.
- **Data Seeding**: Consider how you will seed your database with initial data for development and testing. This can be done through scripts or migration tools.
- **Version Control**: Use version control for your database schema and migrations to track changes over time. Tools like Flyway or Liquibase can help manage database migrations.
- **Backup and Recovery**: Implement a backup strategy to protect your data. Regularly back up your database and test recovery procedures to ensure data integrity.
- **Testing**: Set up a separate testing database to run automated tests without affecting your development or production data.
- **Documentation**: Keep documentation of your database schema, relationships, and any specific configurations to help onboard new developers and maintain the project.
- **Performance Monitoring**: Use monitoring tools to track database performance and identify potential bottlenecks. This is especially important for production databases.
- **Security**: Implement security best practices, such as using strong passwords, restricting access to the database, and encrypting sensitive data.
- **Development Workflow**: Establish a workflow for database changes, including code reviews and testing before deploying changes to production.

##### NOTE

> There is a section for [Development Tools](/util-environment-setup) that provides additional resources and tools for back-end developers.

## Helpful Tools for Back-end Developers

### Object-Relational Mapping (ORM) and Query Builders

| Tool                                | Description                                                                                                                                                                                        |
| ----------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [TypeORM](https://typeorm.io/)      | An ORM for TypeScript and JavaScript that supports various SQL databases. It simplifies database interactions by allowing developers to work with objects instead of raw SQL queries.              |
| [Sequelize](https://sequelize.org/) | A promise-based Node.js ORM for SQL databases. It provides a simple API for database operations and supports transactions, migrations, and associations.                                           |
| [Prisma](https://www.prisma.io/)    | A modern ORM that provides type safety and auto-completion for database queries. It supports various databases and integrates well with TypeScript.                                                |
| [Mongoose](https://mongoosejs.com/) | An ODM (Object Data Mapping) library for MongoDB and Node.js. It provides a schema-based solution to model application data and includes features like validation, middleware, and query building. |
| [Knex.js](https://knexjs.org/)      | A SQL query builder for Node.js that supports multiple SQL databases. It allows developers to write SQL queries in a more programmatic way and supports migrations.                                |

### Database Management GUIs

| Tool                                                        | Description                                                                                                                                                                                                                                                                                 |
| ----------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [DBeaver](https://dbeaver.io/)                              | A universal database management tool that supports various databases, including PostgreSQL, MySQL, and MongoDB. DBeaver provides a user-friendly interface for managing databases, executing queries, and visualizing data. It supports features like ER diagrams, data export, and import. |
| [DataGrip](https://www.jetbrains.com/datagrip/)             | A powerful database IDE from JetBrains that supports multiple databases, including PostgreSQL, MySQL, and MongoDB. DataGrip provides advanced features like code completion, query analysis, and database refactoring.                                                                      |
| [TablePlus](https://tableplus.com/)                         | A modern database management tool that supports multiple databases including PostgreSQL, MySQL, SQLite, and Redis. It provides a clean interface for database operations, query execution, and data visualization.                                                                          |
| [pgAdmin](https://www.pgadmin.org/)                         | A web-based administration tool for PostgreSQL databases. It provides a user-friendly interface for managing PostgreSQL databases, including query execution, schema management, and performance monitoring.                                                                                |
| [MongoDB Compass](https://www.mongodb.com/products/compass) | A graphical user interface for MongoDB that allows you to explore and manage your MongoDB databases. It provides features like schema visualization, query execution, and performance monitoring.                                                                                           |
| [Adminer](https://www.adminer.org/)                         | A lightweight, web-based database management tool written in PHP. It supports multiple database systems including MySQL, PostgreSQL, SQLite, and MongoDB. Adminer provides a simple interface for database administration.                                                                  |
| [phpMyAdmin](https://www.phpmyadmin.net/)                   | A popular web-based administration tool for MySQL and MariaDB databases. It offers a comprehensive interface for database management, including table creation, data manipulation, user management, and backup operations.                                                                  |
| [Sequel Pro](https://www.sequelpro.com/)                    | A fast and easy-to-use database management application for macOS that works with MySQL databases. It provides a native Mac interface for database operations, query execution, and data export/import.                                                                                      |
| [HeidiSQL](https://www.heidisql.com/)                       | A lightweight database management tool for Windows that supports MySQL, PostgreSQL, SQLite, and Microsoft SQL Server. It provides features like query execution, data editing, user management, and database export/import.                                                                 |

### Schema Migration and Version Control

| Tool                                                                                  | Description                                                                                                                                                                                         |
| ------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [Flyway](https://flywaydb.org/)                                                       | A database migration tool that allows developers to version control their database schema. It supports SQL-based migrations and integrates with various build tools.                                |
| [Liquibase](https://www.liquibase.org/)                                               | An open-source database schema change management tool. It allows developers to define database changes in a structured way and supports various databases.                                          |
| [TypeORM Migrations](https://typeorm.io/#/migrations)                                 | Built-in migration support for TypeORM. It allows developers to create, run, and revert database migrations using TypeScript or JavaScript.                                                         |
| [Sequelize Migrations](https://sequelize.org/master/manual/migrations.html)           | Built-in migration support for Sequelize. It provides a CLI tool to create and manage migrations, allowing developers to version control their database schema changes.                             |
| [Prisma Migrations](https://www.prisma.io/docs/concepts/components/prisma-migrations) | Prisma's built-in migration system that allows developers to manage database schema changes. It generates SQL migration files based on the Prisma schema and provides a CLI for running migrations. |

### Backup and Recovery Tools

| Tool                                                                      | Description                                                                                                                                                                                                                                             |
| ------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [pg_dump](https://www.postgresql.org/docs/current/app-pgdump.html)        | A utility for backing up PostgreSQL databases. It allows you to create backups of your database in various formats, including plain SQL scripts and custom formats. Supports options for excluding specific tables or schemas.                          |
| [mongodump](https://docs.mongodb.com/manual/reference/program/mongodump/) | A utility for backing up MongoDB databases. It creates binary backups of your MongoDB collections and can be used to restore data using the `mongorestore` command. Supports options for filtering collections and documents.                           |
| [mysqldump](https://dev.mysql.com/doc/refman/8.0/en/mysqldump.html)       | A command-line utility for creating logical backups of MySQL databases. It generates SQL statements that can recreate the database structure and data. Supports various options for customizing backups, including single transactions and compression. |
| [Redis BGSAVE](https://redis.io/commands/bgsave/)                         | A Redis command that creates a snapshot of the dataset in the background. It saves the current state of the Redis database to a .rdb file without blocking the server. Useful for creating point-in-time backups of in-memory data.                     |

### Testing Frameworks and Tools

| Tool                            | Description                                                                                                                                                                                                                                                                  |
| ------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [Jest](https://jestjs.io/)      | A JavaScript testing framework that works well with Node.js applications. It supports unit testing, integration testing, and snapshot testing. Jest can be used to test database interactions by mocking database calls or using an in-memory database.                      |
| [Mocha](https://mochajs.org/)   | A flexible JavaScript test framework for Node.js applications. It supports various assertion libraries and can be used for unit testing and integration testing. Mocha can be combined with libraries like Chai for assertions and Sinon for mocking database calls.         |
| [Chai](https://www.chaijs.com/) | An assertion library for Node.js and browsers. It can be used with testing frameworks like Mocha to write expressive tests for database interactions. Chai provides various assertion styles, including BDD (Behavior-Driven Development) and TDD (Test-Driven Development). |

### Performance Monitoring and Analytics

| Tool                                 | Description                                                                                                                                                                                                                                                                                                                                        |
| ------------------------------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [Prometheus](https://prometheus.io/) | An open-source monitoring and alerting toolkit. It can be used to monitor database performance by collecting metrics from your database and visualizing them in dashboards. Prometheus supports various exporters for different databases, allowing you to track metrics like query performance, connection counts, and error rates.               |
| [Grafana](https://grafana.com/)      | An open-source analytics and monitoring platform that integrates with Prometheus and other data sources. It allows you to create custom dashboards to visualize database metrics and performance data. Grafana supports various plugins for different databases, enabling you to monitor query performance, resource usage, and other key metrics. |

### Load Testing and Benchmarking

| Tool                                                            | Description                                                                                                                                                                                                                                          |
| --------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [pgbench](https://www.postgresql.org/docs/current/pgbench.html) | A benchmarking tool included with PostgreSQL that can be used to test database performance under various load conditions. It provides built-in tests and allows custom scenarios to measure transaction throughput and latency.                      |
| [sysbench](https://github.com/akopytov/sysbench)                | A multi-threaded benchmark tool that can test various system parameters including database performance. It supports MySQL, PostgreSQL, and other databases, providing tests for OLTP workloads, file I/O, and CPU performance.                       |
| [Apache JMeter](https://jmeter.apache.org/)                     | A popular open-source tool for load testing that can be used to test database performance through JDBC connections. JMeter allows you to create complex test scenarios and measure database response times under various load conditions.            |
| [Artillery](https://artillery.io/)                              | A modern load testing toolkit that can be used to test APIs and database-backed applications. It supports HTTP, WebSocket, and Socket.io protocols and can be used to simulate realistic user traffic patterns against database-driven applications. |

### Documentation and Schema Visualization

| Tool                                      | Description                                                                                                                                                                                                                                                                                      |
| ----------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| [dbdocs.io](https://dbdocs.io/)           | A platform for creating and sharing database documentation. It allows you to generate interactive documentation from your database schema, making it easy for team members to understand the database structure. Supports collaboration features and version control for database documentation. |
| [SchemaSpy](http://schemaspy.org/)        | An open-source tool that generates database documentation by analyzing database metadata. It creates HTML documentation with entity relationship diagrams, table details, and foreign key relationships. SchemaSpy supports multiple database types and can be integrated into CI/CD pipelines.  |
| [ERDPlus](https://erdplus.com/)           | A web-based tool for creating Entity Relationship Diagrams (ERDs). It allows you to design database schemas visually and export them to various formats. ERDPlus supports collaborative editing and can generate SQL DDL statements from your diagrams.                                          |
| [Lucidchart](https://www.lucidchart.com/) | A web-based diagramming application that can be used to create database diagrams and ERDs. It offers collaborative features, templates, and integration with various tools. Lucidchart supports real-time collaboration and can import/export diagrams in multiple formats.                      |

### Development Environment Setup

| Tool                                               | Description                                                                                                                                                                                                                                                                        |
| -------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [Docker](https://www.docker.com/)                  | A containerization platform that allows you to package databases and applications into portable containers. Docker is excellent for creating consistent development environments and can be used to run databases like PostgreSQL, MySQL, and MongoDB in isolated containers.      |
| [Docker Compose](https://docs.docker.com/compose/) | A tool for defining and running multi-container Docker applications. Perfect for setting up development environments with multiple services (database, application, cache, etc.). Docker Compose files make it easy to share and reproduce development setups across team members. |
| [Vagrant](https://www.vagrantup.com/)              | A tool for building and managing virtual machine environments. Useful for creating reproducible development environments that include databases and other dependencies. Vagrant can provision VMs with specific database configurations using provisioning scripts.                |
| [XAMPP](https://www.apachefriends.org/)            | A cross-platform web server solution that includes Apache, MySQL, PHP, and Perl. XAMPP provides an easy way to set up a local development environment for web applications that use MySQL databases. It's particularly popular for PHP development.                                |
| [MAMP](https://www.mamp.info/)                     | Similar to XAMPP but specifically designed for macOS and Windows. MAMP provides Apache, MySQL, and PHP in a single package, making it easy to set up local development environments for web applications.                                                                          |
| [LocalStack](https://localstack.cloud/)            | A fully functional local AWS cloud stack that can simulate AWS services including DynamoDB, RDS, and other database services. Useful for developing applications that use AWS database services without incurring cloud costs during development.                                  |

## Next Steps

### Immediate Actions

| Priority   | Action                                                                                                               | Purpose                                                   |
| ---------- | -------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------- |
| **High**   | [Database Design](/db-getting-started-with-database-design/fundamentals)                                             | Master core database concepts and design principles       |
| **Medium** | [Database Normalization And Relationships](/db-getting-started-with-database-design/normalization-and-relationships) | Learn to structure data efficiently and reduce redundancy |
| **Medium** | [Database Schema Design and Migration](/db-getting-started-with-database-design/schema-design-and-migration)         | Apply design patterns for scalable database architecture  |

<BackToTop />
```
